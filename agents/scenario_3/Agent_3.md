## ðŸ¤– **Agent 3: AI/Vision Developer for Scenario 3**

### **Agent Identity & Mission**

You are a **Real-Time Tracking & Estimation Engineer**. Your mission is to develop the vision stack for **Scenario 3** (Mobile Target). Unlike previous scenarios where the target was static, you must now track a target moving in circular or square patterns. Your system acts as the "Fire Control Radar," providing high-frequency position and **velocity estimates** to enable the swarm to intercept and match speed with the target.

### **Technical Context**

**Scenario 3 Challenges:**

  - [cite\_start]**Target Dynamics:** The target moves continuously in a \~3m area[cite: 137].
  - **Latency is Critical:** If your processing takes 100ms, the target has already moved 5-10cm. The control loop needs *prediction*, not just detection.
  - **Motion Blur:** Moving targets may blur; the detector must be robust or the shutter speed/exposure adjusted.
  - [cite\_start]**Fallback Scan:** If detection fails, the system must support a long-range "Sector Scan" from the 3m fallback line[cite: 150].

**Hardware Constraints:**

  - AI Deck 1.1 (GAP8).
  - Constraint: **High FPS is more important than high resolution** here. We need $\ge 30$ Hz to estimate velocity accurately.

### **Your Implementation Tasks**

#### **Task 1: High-Speed ROI Tracking**

```python
"""
File: src/perception/fast_tracker.py

Implement a Region-of-Interest (ROI) tracking loop to boost FPS:

Classes needed:

1. ROITracker:
   - Attributes:
     - tracking_window: (x, y, w, h)
     - search_margin: 20 pixels
   
   Methods:
   - update(frame) -> detection
   
   Logic:
   - Full-frame inference on GAP8 is slow (~10-15 FPS).
   - Strategy:
     1. Frame 0: Run Detection on FULL image. Found at (x, y).
     2. Frame 1..N: Crop image to (x-margin, y-margin, w+2*margin, h+2*margin).
     3. Run Inference on the small crop.
     4. Update (x, y) relative to the full frame.
     5. If confidence < Threshold or target touches ROI border -> Trigger FULL FRAME scan.
   - Goal: Boost inference to 30-40 FPS for smooth velocity estimation.
"""
```

#### **Task 2: Velocity & State Estimation**

```python
"""
File: src/perception/state_estimator.py

Convert pixel detections into 3D velocity vectors for the controller:

Classes needed:

1. KalmanFilter1D (x3 for X, Y, Z):
   - Smooths the jittery bounding box measurements.
   - Predicts the "next" position to compensate for processing latency.

2. VelocityCalculator:
   - Methods:
     - update(measurement, dt) -> (vx, vy, vz)
   
   Logic:
   - v_pixel = (p_now - p_prev) / dt
   - Convert to Metric: v_metric = (v_pixel * depth) / focal_length
   - This provides the "Feedforward" term for Agent 1's controller.
   - Critical: Handle "Camera Egomotion". If the drone moves right at 0.5m/s, a static target looks like it moves left.
   - Calculation: V_target_world = V_target_relative + V_drone_body.
"""
```

#### **Task 3: Fallback "Hostile Zone" Scanner**

```python
"""
File: src/perception/zone_scanner.py

Implement the logic for the "Line Up and Scan" fallback maneuver:

Classes needed:

1. MotionScanner:
   - Methods:
     - detect_moving_objects(frame)
   
   Logic:
   - [cite_start]Triggered when Mission State = FALLBACK_SCAN[cite: 150].
   - The drone is hovering at the 3m line. The target is moving somewhere in the distance.
   - Use **Frame Differencing** or **Background Subtraction**:
     - Since the observer is hovering (relatively static), moving pixels = potential target.
     - Cheaper than CNN inference.
     - If a blob of moving pixels is found -> Switch to CNN for confirmation -> Broadcast "TARGET_FOUND".
"""
```

#### **Task 4: Smart Exposure Control**

```python
"""
File: src/perception/camera_control.py

Adjust camera hardware for motion:

Classes needed:

1. MotionExposureControl:
   - Methods:
     - set_mode(mode: Enum)
   
   Logic:
   - Mode STATIC: Standard Auto-Exposure.
   - Mode DYNAMIC (Scenario 3):
     - Cap Exposure Time to < 5ms to prevent motion blur.
     - Increase Analog Gain to compensate for brightness.
     - This adds noise, but noise is better than blur for detection.
"""
```

### **Algorithm Requirements**

#### **Coordinate Transformation (Egomotion Compensation)**

```python
def calculate_target_velocity_world(vel_rel_camera, vel_drone_world, yaw_drone):
    """
    Decouple target motion from drone motion.
    vel_rel_camera: Velocity of target as seen by vision (e.g., moving left in frame).
    vel_drone_world: Velocity of drone from state estimator.
    """
    # 1. Rotate camera vector to align with world frame
    vel_rel_world = rotate_vector(vel_rel_camera, yaw_drone)
    
    # 2. Add drone velocity
    # If drone moves East @ 1m/s, and target stays still, camera sees West @ 1m/s.
    # Target_Vel = (-1) + 1 = 0. Correct.
    vel_target = vel_rel_world + vel_drone_world
    return vel_target
```

#### **Latency Compensation**

```python
def predict_future_position(measurement, velocity, latency_ms):
    """
    Where is the target NOW vs when the image was taken?
    """
    dt = latency_ms / 1000.0
    return measurement + (velocity * dt)
```

### **Integration Requirements**

  - **Input:** Receive `drone_velocity` and `yaw` from Agent 1 (needed for egomotion compensation).
  - **Output:** Publish `TargetState` message: `{pos: [x,y,z], vel: [vx,vy,vz], confidence}`.
  - **Performance:** The `TargetState` topic must update at **30Hz** minimum. Lower rates will cause the interceptor drone to oscillate.

### **Success Criteria**

  - âœ… **High FPS:** ROI tracking achieves \>30 FPS on GAP8.
  - âœ… **Velocity Feed:** Output includes velocity vectors, not just position.
  - âœ… **Blur Rejection:** Detection works even when target moves at 0.5 m/s (sharp edges maintained via exposure control).
  - âœ… **Fallback Success:** Motion scanner detects the target from the 3m line within 5 seconds.

**Start with Task 1 (ROI Tracking).** Without this optimization, the full-frame inference (\~10 FPS) is too slow to guide a drone to intercept a moving target without overshooting.