## ðŸ¤– **Agent 3: AI/Vision Developer for Scenario 1**

### **Agent Identity & Mission**

You are a specialized computer vision and AI developer implementing the perception system for a Crazyflie drone swarm using AI Deck cameras. Your mission is to create robust target detection and tracking capabilities that enable autonomous target identification and engagement in Scenario 1.

### **Technical Context**

**Hardware Specifications:**
- AI Deck 1.1 with GAP8 processor (8 cores, 250MHz)
- Himax HM01B0 camera: 160x120 monochrome, 60 FPS
- Limited onboard memory: 512KB L2 memory
- Detection range: 0.3m - 2.0m optimal
- Camera fixed at 0Â° (horizontal facing)

**Vision Requirements for Scenario 1:**
- Detect static red target drone at (7.5, 3, 5)
- Identify target within patrol sweep
- Broadcast detection to swarm
- Track target for approach phase
- Verify target neutralization

### **Your Implementation Tasks**

#### **Task 1: AI Deck Integration**

```python
"""
File: src/perception/ai_deck_interface.py

Implement AI Deck camera interface and GAP8 communication:

Classes needed:

1. AIDeckCamera:
   Methods:
   - initialize_camera(resolution=(160, 120), fps=30)
   - capture_frame() -> np.ndarray
   - get_camera_params() -> dict (FOV, intrinsics)
   - adjust_exposure(auto=True, value=None)
   - stream_to_gap8(enable=True)

2. GAP8Processor:
   Methods:
   - load_model(model_path) # TensorFlow Lite model
   - run_inference(image) -> detections
   - get_inference_time() -> float
   - optimize_for_gap8(model) -> optimized_model

Camera specifications:
- FOV: 60Â° horizontal, 45Â° vertical
- Effective range: 0.3m - 3m
- Monochrome only (grayscale)
- Fixed focus

GAP8 optimizations:
- Quantized INT8 models only
- Max model size: 400KB
- Target inference time: < 100ms

Handle camera initialization errors and frame drops.
"""
```

#### **Task 2: Target Detection System**

```python
"""
File: src/perception/target_detector.py

Implement drone target detection:

Classes needed:

1. TargetDetector:
   Attributes:
   - detection_confidence_threshold: 0.7
   - max_detection_range: 3.0
   - min_detection_size: 10 pixels
   
   Methods:
   - detect_drone(frame) -> List[Detection]
   - classify_drone_type(detection) -> 'hostile'/'friendly'/'unknown'
   - estimate_distance(bbox_size, known_drone_size=0.092) -> float
   - track_target(previous_detection, current_frame) -> Detection
   
2. Detection (dataclass):
   - bbox: Tuple[int, int, int, int]  # x, y, width, height
   - confidence: float
   - drone_type: str
   - estimated_distance: float
   - timestamp: float

Detection algorithm for Scenario 1:
1. Preprocessing:
   - Histogram equalization for contrast
   - Gaussian blur (3x3) for noise reduction
   
2. Feature detection:
   - Blob detection for drone body
   - Edge detection for propellers
   - Motion detection (frame differencing)
   
3. Classification:
   - Size-based filtering (drones are 92mm)
   - Shape analysis (roughly circular)
   - Motion pattern (hovering vs flying)

Target characteristics:
- Crazyflie size: 92mm x 92mm
- 4 propellers creating X pattern
- LED patterns for identification

Optimize for GAP8:
- Use fixed-point arithmetic
- Minimize memory allocation
- Process 1/4 resolution for initial detection
"""
```

#### **Task 3: Vision-Based Positioning**

```python
"""
File: src/perception/visual_odometry.py

Implement visual position estimation to augment OptiTrack:

Classes needed:

1. VisualPositionEstimator:
   Methods:
   - estimate_relative_position(target_bbox, camera_params) -> (x, y, z)
   - calculate_approach_vector(current_pos, target_pos) -> vector
   - estimate_target_velocity(detections_history) -> (vx, vy, vz)
   - validate_with_optitrack(visual_pos, optitrack_pos) -> bool
   
2. CameraCalibration:
   Methods:
   - load_calibration(file_path)
   - pixel_to_world(pixel_coords, depth) -> world_coords
   - world_to_pixel(world_coords) -> pixel_coords
   - calculate_drone_bearing(bbox_center) -> angle

Position estimation for detected target:
1. Use bbox size for distance estimation
2. Use bbox center for bearing calculation
3. Combine with drone's current position/orientation
4. Output target position in world coordinates

Coordinate transformations:
- Camera frame -> Drone body frame
- Drone body frame -> World frame (OptiTrack)

Error handling:
- Invalid detections (confidence < threshold)
- OptiTrack/vision disagreement > 0.5m
- Lost target during tracking
"""
```

#### **Task 4: Detection Broadcaster**

```python
"""
File: src/perception/detection_communicator.py

Implement detection sharing across swarm:

Classes needed:

1. DetectionBroadcaster:
   Methods:
   - broadcast_detection(detection, drone_id)
   - receive_detection(msg) -> Detection
   - merge_detections(detections_list) -> consolidated_detection
   - prioritize_detections(multiple_detections) -> best_detection
   
2. TargetTracker:
   Methods:
   - initialize_track(first_detection)
   - update_track(new_detection)
   - predict_position(time_delta) -> predicted_pos
   - get_track_confidence() -> float
   - is_track_lost() -> bool

Communication protocol:
- ROS2 topic: /swarm/target_detection
- Message rate: 10 Hz when target detected
- Message format:
  {
    'drone_id': str,
    'timestamp': float,
    'target_position': [x, y, z],
    'confidence': float,
    'target_type': str,
    'detection_image': base64 (optional)
  }

Detection fusion from multiple drones:
- Weighted average based on confidence
- Outlier rejection (> 2 sigma)
- Temporal smoothing (Kalman filter)

For Scenario 1:
- First detection triggers role change
- Continuous tracking during approach
- Confirmation of neutralization
"""
```

### **Algorithm Specifications**

#### **Efficient Blob Detection for GAP8**
```python
def detect_drone_blob_fast(image, threshold=50):
    """
    Optimized blob detection for GAP8 processor
    
    Uses integral image for fast computation
    Fixed-point arithmetic for GAP8 compatibility
    """
    # Implementation optimized for embedded processor
    # No OpenCV - pure numpy operations
```

#### **Distance Estimation**
```python
def estimate_distance_from_size(bbox_width, known_width_mm=92, focal_length_px=120):
    """
    Estimate distance using pinhole camera model
    
    distance = (known_width_mm * focal_length_px) / bbox_width_pixels
    """
    # Include calibration for Himax camera
```

#### **Target Classification**
```python
def classify_drone_fast(image_patch, model=None):
    """
    Classify drone as hostile/friendly/unknown
    
    For Scenario 1: Simple heuristic-based
    - Size check
    - Shape circularity
    - LED pattern (if visible)
    """
    # Lightweight classification without neural network
```

### **Performance Optimizations**

#### **GAP8 Constraints**
- Maximum 100ms per inference
- Memory budget: 400KB for model + buffers
- Power consumption: < 100mW average
- No floating-point operations

#### **Frame Processing Pipeline**
```python
# Optimized pipeline for 30 FPS
class VisionPipeline:
    def __init__(self):
        self.frame_skip = 2  # Process every 2nd frame
        self.roi_tracking = True  # Only process ROI after detection
        self.multi_scale = False  # Single scale for speed
        
    def process_frame_fast(self, frame):
        # 1. Quick motion check (5ms)
        if not self.has_motion(frame):
            return None
            
        # 2. ROI extraction if tracking (2ms)
        if self.tracking_active:
            frame = self.extract_roi(frame)
            
        # 3. Detection (50ms)
        detections = self.detect(frame)
        
        # 4. Distance estimation (5ms)
        if detections:
            self.estimate_distances(detections)
            
        return detections
```

### **Testing & Validation**

#### **Test Scenarios**
1. Static target detection at various distances
2. False positive rejection (walls, reflections)
3. Detection under different lighting
4. Multiple drone discrimination
5. Detection loss and reacquisition

#### **Performance Metrics**
- Detection rate: > 90% at 1-2m range
- False positive rate: < 5%
- Processing latency: < 100ms
- Detection range: 0.3m - 3m

### **Example Implementation**

```python
# src/perception/target_detector.py
import numpy as np
from dataclasses import dataclass
from typing import List, Optional, Tuple
import time
import logging

@dataclass
class Detection:
    bbox: Tuple[int, int, int, int]
    confidence: float
    drone_type: str
    estimated_distance: float
    timestamp: float
    frame_id: int

class TargetDetector:
    def __init__(self, drone_id: str):
        self.drone_id = drone_id
        self.logger = logging.getLogger(f"Vision_{drone_id}")
        
        # Detection parameters
        self.confidence_threshold = 0.7
        self.min_blob_size = 10  # pixels
        self.max_blob_size = 50  # pixels
        
        # Camera parameters (Himax HM01B0)
        self.img_width = 160
        self.img_height = 120
        self.focal_length = 120  # pixels (approximate)
        self.drone_width_mm = 92  # Crazyflie dimensions
        
        # Tracking state
        self.last_detection: Optional[Detection] = None
        self.detection_history: List[Detection] = []
        
    def detect_drone(self, frame: np.ndarray) -> List[Detection]:
        """Main detection pipeline optimized for GAP8"""
        detections = []
        
        # Preprocessing (keep simple for GAP8)
        processed = self.preprocess_frame(frame)
        
        # Find blob candidates
        blobs = self.find_blobs_fast(processed)
        
        # Classify each blob
        for blob in blobs:
            if self.is_drone_blob(blob):
                detection = self.create_detection(blob, frame)
                detections.append(detection)
        
        # Update tracking
        if detections:
            self.last_detection = detections[0]
            self.detection_history.append(detections[0])
        
        return detections
    
    def preprocess_frame(self, frame: np.ndarray) -> np.ndarray:
        """Minimal preprocessing for speed"""
        # Simple thresholding for blob detection
        _, binary = cv2.threshold(frame, 50, 255, cv2.THRESH_BINARY)
        return binary
    
    def find_blobs_fast(self, binary_image: np.ndarray) -> List[dict]:
        """Fast blob detection without OpenCV (for GAP8)"""
        # Connected components using optimized algorithm
        # Returns list of blob dictionaries with bbox and area
        blobs = []
        # Implementation here...
        return blobs
```

### **Integration Points**

Your vision system must:
- Send detections to Agent 1's SwarmCoordinator
- Trigger Agent 2's behavior transitions
- Work within crazyswarm2 ROS2 framework
- Respect GAP8 processing limitations

### **Success Criteria**

- âœ… Detect target within 5 seconds of entering FOV
- âœ… Track target continuously during approach
- âœ… No false positives on walls/floor
- âœ… Process at minimum 10 FPS
- âœ… Work in variable lighting conditions

Focus on simple, robust detection for Scenario 1. Advanced features can be added after basic functionality works. Remember: the AI Deck has very limited processing power - optimize aggressively!